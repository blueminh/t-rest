{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "def get_filenames_from_model(model_name, common_size, gamma, delta, other_schemes=None):\n",
    "    # Define the search pattern, allowing any string for sample size part\n",
    "    search_pattern_non_watermark = f\"{model_name}_*_non-watermark.csv\"\n",
    "    search_pattern_watermark = f\"{model_name}_*_with-watermark_gamma-{gamma}_delta-{delta}.csv\"\n",
    "\n",
    "    # Use glob to get all files matching the search\n",
    "    directory = \"/Users/minhkau/Documents/TUDelft/Year 3/RP/Code/tabular-gpt/samples\"\n",
    "    files_non_watermark = glob.glob(search_pattern_non_watermark, root_dir=directory)\n",
    "    files_watermark = glob.glob(search_pattern_watermark, root_dir=directory)\n",
    "\n",
    "    # Regular expressions to extract sample size from filenames\n",
    "    regex_pattern_non_watermark = re.compile(rf\"{model_name}_(\\d+)_non-watermark.csv\")\n",
    "    regex_pattern_watermark = re.compile(rf\"{model_name}_(\\d+)_with-watermark_gamma-{gamma}_delta-{delta}.csv\")\n",
    "\n",
    "    largest_sample_size_non_watermark = -1\n",
    "    largest_sample_size_with_watermark = -1\n",
    "    largest_file_non_watermark = None\n",
    "    largest_file_with_watermark = None\n",
    "    # Iterate over the matching files and extract sample size\n",
    "    for file in files_non_watermark:\n",
    "        match = regex_pattern_non_watermark.match(os.path.basename(file))\n",
    "        if match:\n",
    "            sample_size = int(match.group(1))\n",
    "            if sample_size == common_size:\n",
    "                largest_file_non_watermark = file\n",
    "                break\n",
    "            if sample_size > largest_sample_size_non_watermark:\n",
    "                largest_sample_size_non_watermark = sample_size\n",
    "                largest_file_non_watermark = file\n",
    "\n",
    "    for file in files_watermark:\n",
    "        match = regex_pattern_watermark.match(os.path.basename(file))\n",
    "        if match:\n",
    "            sample_size = int(match.group(1))\n",
    "            if sample_size == common_size:\n",
    "                largest_file_with_watermark = file\n",
    "                break\n",
    "            if sample_size > largest_sample_size_with_watermark:\n",
    "                largest_sample_size_with_watermark = sample_size\n",
    "                largest_file_with_watermark = file\n",
    "                \n",
    "    if other_schemes:\n",
    "        other_schemes_file_names = []\n",
    "        for scheme_name in other_schemes:\n",
    "            search_pattern_scheme = f\"{model_name}_*_with-watermark_{scheme_name}.csv\"\n",
    "            regex_pattern_scheme = re.compile(rf\"{model_name}_(\\d+)_with-watermark_{scheme_name}.csv\")\n",
    "            files_scheme = glob.glob(search_pattern_scheme, root_dir=directory)\n",
    "            largest_sample_size_scheme = -1\n",
    "            largest_file_sheme = None\n",
    "            for file in files_scheme:\n",
    "                match = regex_pattern_scheme.match(os.path.basename(file))\n",
    "                if match:\n",
    "                    sample_size = int(match.group(1))\n",
    "                    if sample_size == common_size:\n",
    "                        largest_file_sheme = file\n",
    "                        break\n",
    "                    if sample_size > largest_sample_size_scheme:\n",
    "                        largest_sample_size_scheme = sample_size\n",
    "                        largest_file_sheme = file\n",
    "            other_schemes_file_names.append(largest_file_sheme)\n",
    "        return f\"{model_name}.csv\", largest_file_non_watermark, largest_file_with_watermark, other_schemes_file_names\n",
    "            \n",
    "    return f\"{model_name}.csv\", largest_file_non_watermark, largest_file_with_watermark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T14:38:48.981566Z",
     "start_time": "2024-06-08T14:38:48.977095Z"
    }
   },
   "id": "c76c125bfec8f652",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes.csv\n",
      "diabetes_1000_non-watermark.csv\n",
      "diabetes_1000_with-watermark_gamma-0.25_delta-2.0.csv\n",
      "0.5260416666666666\n",
      "0.3210702341137124\n",
      "0.3435374149659864\n"
     ]
    }
   ],
   "source": [
    "from sdmetrics.single_table import MLPRegressor, LinearRegression, LogisticDetection, BinaryDecisionTreeClassifier, BinaryMLPClassifier, LinearRegression, MLPRegressor\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "real_file_name, no_water_mark_file_name, with_water_mark_file_name = get_filenames_from_model('diabetes', common_size=1000, gamma=0.25, delta=2.0)\n",
    "target = 'Outcome'\n",
    "\n",
    "\n",
    "print(real_file_name)\n",
    "print(no_water_mark_file_name)\n",
    "print(with_water_mark_file_name)\n",
    "\n",
    "samples_dir = \"/Users/minhkau/Documents/TUDelft/Year 3/RP/Code/tabular-gpt/samples/\"\n",
    "real_path = samples_dir + real_file_name\n",
    "synth_no_watermark_path = samples_dir + no_water_mark_file_name\n",
    "synth_with_watermark_path = samples_dir + with_water_mark_file_name\n",
    "\n",
    "real_table = pd.read_csv(real_path)\n",
    "non_watermark_table = pd.read_csv(synth_no_watermark_path)\n",
    "watermark_table = pd.read_csv(synth_with_watermark_path)\n",
    "\n",
    "common_size = 1000\n",
    "test_size = 200\n",
    "frac = 0.8\n",
    "\n",
    "# real_table = real_table.sample(common_size)\n",
    "# non_watermark_table = non_watermark_table.sample(common_size)\n",
    "# watermark_table = watermark_table.sample(common_size)\n",
    "\n",
    "test_real = real_table.sample(test_size)\n",
    "real_table = real_table.drop(test_real.index)\n",
    "# train_real = real_table.sample(common_size)\n",
    "train_real = real_table\n",
    "train_synth_no_W = non_watermark_table\n",
    "train_synth_W = watermark_table\n",
    "\n",
    "# classification task: decision tree\n",
    "print(BinaryDecisionTreeClassifier.compute(train_real, test_real, target=target))\n",
    "print(BinaryDecisionTreeClassifier.compute(train_synth_no_W, test_real, target=target))\n",
    "print(BinaryDecisionTreeClassifier.compute(train_synth_W, test_real, target=target))\n",
    "\n",
    "# regression task\n",
    "# print(LinearRegression.compute(train_real, test_real, target=target))\n",
    "# print(LinearRegression.compute(train_synth_no_W, test_real, target=target))\n",
    "# print(LinearRegression.compute(train_synth_W, test_real, target=target))\n",
    "# print(\"ahhhhhh\")\n",
    "# print(MLPRegressor.compute(train_real, test_real, target=target))\n",
    "# print(MLPRegressor.compute(train_synth_no_W, test_real, target=target))\n",
    "# print(MLPRegressor.compute(train_synth_W, test_real, target=target))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T14:52:59.564190Z",
     "start_time": "2024-06-08T14:52:59.535470Z"
    }
   },
   "id": "d5a12741f13c6d2d",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def preprocess_data(train, test, target, label_encoder = None):\n",
    "    X_train = train.drop(target, axis=1)\n",
    "    y_train = train[target]   \n",
    "    \n",
    "    X_test = test.drop(target, axis=1)\n",
    "    y_test = test[target]\n",
    "    \n",
    "    if label_encoder:\n",
    "        y_train = label_encoder.fit_transform(y_train)\n",
    "        y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "    \n",
    "    numeric_features = X_train.select_dtypes(include=['int', 'float']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, preprocessor\n",
    "    \n",
    "# this works for adult and diabetes\n",
    "def logistic_classifier(train, test, target):\n",
    "    X_train, y_train, X_test, y_test, preprocessor = preprocess_data(train, test, target, label_encoder=LabelEncoder())\n",
    "    \n",
    "    # Create a pipeline with preprocessing and the classifier\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', LogisticRegression())])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, predictions)  # '>50K' as the positive label\n",
    "    # print(\"F1 Score:\", f1)\n",
    "    return f1\n",
    "\n",
    "# this works for adult and diabetes\n",
    "def linear_classifier(train, test, target):\n",
    "    X_train, y_train, X_test, y_test, preprocessor = preprocess_data(train, test, target, label_encoder=LabelEncoder())\n",
    "\n",
    "    # Create a pipeline with preprocessing and the classifier\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', LinearRegression())])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities (continuous values) and threshold them to get binary predictions\n",
    "    predictions_prob = pipeline.predict(X_test)\n",
    "    threshold = 0.5\n",
    "    predictions = (predictions_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    # print(\"F1 Score:\", f1)\n",
    "    return f1\n",
    "\n",
    "def linear_regression(train, test, target):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, preprocessor = preprocess_data(train, test, target)\n",
    "\n",
    "    # Create a pipeline with preprocessing and the classifier\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', LinearRegression())])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate regression metrics\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    # print(\"Mean Squared Error (MSE):\", mse)\n",
    "    # print(\"R-squared (R²):\", r2)\n",
    "    # \n",
    "    return mse\n",
    "\n",
    "def logistic_regression(train, test, target):\n",
    "    X_train, y_train, X_test, y_test, preprocessor = preprocess_data(train, test, target)\n",
    "    # Create a pipeline with preprocessing and the classifier\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', LogisticRegression(max_iter=1000))])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate regression metrics\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    # print(\"Mean Squared Error (MSE):\", mse)\n",
    "    # print(\"R-squared (R²):\", r2)\n",
    "    # \n",
    "    return mse"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T15:42:04.074106Z",
     "start_time": "2024-06-08T15:42:04.069743Z"
    }
   },
   "id": "8f2cad3c1a415bde",
   "execution_count": 244
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7474747474747475\n",
      "0.7169811320754716\n",
      "0.6666666666666666\n",
      "0.6966292134831461\n",
      "0.6875\n",
      "0.5747126436781609\n"
     ]
    }
   ],
   "source": [
    "real_file_name, no_water_mark_file_name, with_water_mark_file_name = get_filenames_from_model('adult', common_size=1000, gamma=0.25, delta=2.0)\n",
    "target = 'class'\n",
    "\n",
    "samples_dir = \"/Users/minhkau/Documents/TUDelft/Year 3/RP/Code/tabular-gpt/samples/\"\n",
    "real_path = samples_dir + real_file_name\n",
    "synth_no_watermark_path = samples_dir + no_water_mark_file_name\n",
    "synth_with_watermark_path = samples_dir + with_water_mark_file_name\n",
    "\n",
    "real_table = pd.read_csv(real_path)\n",
    "non_watermark_table = pd.read_csv(synth_no_watermark_path)\n",
    "watermark_table = pd.read_csv(synth_with_watermark_path)\n",
    "\n",
    "common_size = 1000\n",
    "test_size = 200\n",
    "frac = 0.8\n",
    "\n",
    "# real_table = real_table.sample(common_size)\n",
    "non_watermark_table = non_watermark_table.sample(common_size)\n",
    "watermark_table = watermark_table.sample(common_size)\n",
    "\n",
    "\n",
    "test_real = real_table.sample(test_size)\n",
    "real_table = real_table.drop(test_real.index)\n",
    "# train_real = real_table.sample(common_size)\n",
    "train_real = real_table\n",
    "train_synth_no_W = non_watermark_table\n",
    "train_synth_W = watermark_table\n",
    "\n",
    "\n",
    "print(logistic_classifier(real_table, test_real, target))\n",
    "print(logistic_classifier(non_watermark_table, test_real, target))\n",
    "print(logistic_classifier(watermark_table, test_real, target))\n",
    "\n",
    "print(linear_classifier(real_table, test_real, target))\n",
    "print(linear_classifier(non_watermark_table, test_real, target))\n",
    "print(linear_classifier(watermark_table, test_real, target))\n",
    "\n",
    "# print(linear_regression(real_table, test_real, target))\n",
    "# print(linear_regression(non_watermark_table, test_real, target))\n",
    "# print(linear_regression(watermark_table, test_real, target))\n",
    "# \n",
    "# \n",
    "# print(logistic_regression(real_table, test_real, target))\n",
    "# print(logistic_regression(non_watermark_table, test_real, target))\n",
    "# print(logistic_regression(watermark_table, test_real, target))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T15:41:32.898895Z",
     "start_time": "2024-06-08T15:41:32.268107Z"
    }
   },
   "id": "b4d696faa0dd7bd8",
   "execution_count": 243
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
